{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding and Representation Learning\n",
    "\n",
    "## Learning Lower-Dimensional Representations\n",
    "\n",
    "In the previous chapter, we motivated the convolutional architecture using a simple argument.  The larger our input vector, the larger our model. Large models with lots of parameters are expressive, but they’re also increasingly data hungry. This means that without sufficiently large volumes of training data, we will likely overfit. Convolutional architectures help us cope with the curse of dimensionality by reducing the number of parameters in our models without necessarily diminishing expressiveness.\n",
    "\n",
    "Regardless, convolutional networks still require large amounts of labeled training data. And for many problems, labeled data is scarce and expensive to generate. Our goal is to develop effective learning models in situations where labeled data is scarce but wild, unlabeled data is plentiful. We’ll approach this problem by learning embeddings, or low-dimensional representations, in an unsupervised fashion. Because these unsupervised models allow us to offload all of the heavy lifting of automated feature selection, we can use the generated embeddings to solve learning problems using smaller models that require less data. This process is summarized in Figure given below.\n",
    "\n",
    "<img src='images/img1.PNG'>\n",
    "\n",
    "In  the process of developing algorithms that learn good embeddings, we’ll also explore other applications of learning lower-dimensional representations, such as visualization and semantic hashing. We’ll start by considering situations where all of the important information is already contained within the original input vector itself. In this case, learning embeddings is equivalent to developing an effective compression algorithm. \n",
    "\n",
    "In the next section, we’ll introduce principal component analysis (PCA), a classic method for  dimensionality reduction. In subsequent sections, we’ll explore more powerful neural methods for learning compressive embeddings. \n",
    "\n",
    "# Principal Component Analysis\n",
    "\n",
    "The basic concept behind PCA is that we’d like to find a set of axes that communicates the most information about our dataset. More specifically, if we have ddimensional data, we’d like to find a new set of m < d dimensions that conserves as much valuable information from the original dataset. For simplicity, let’s choose d = 2,m = 1. Assuming that variance corresponds to information, we can perform this transformation through an iterative process. First we find a unit vector along which the dataset has maximum variance.  Because this direction contains the most information, we select this direction as our first axis. Then from the set of vectors orthogonal to this first choice, we pick a new unit vector along which the dataset has maximum variance. This is our second axis. We continue this process until we have found a total of d new vectors that represent new axes. We project our data onto this new set of axes.  We then decide a good value for m and toss out all but the first m axes (the principal components, which store the most information). The result is shown in Figure given below.\n",
    "\n",
    "<img src='images/img2.PNG'>\n",
    "\n",
    "For the mathematically initiated, we can view this operation as a project onto the vector space spanned by the top m eigenvectors of the dataset’s covariance matrix (within constant scaling). Let us represent the dataset as a matrix X with dimensions n×d (i.e., n inputs of d dimensions). We’d like to create an embedding matrix T with dimensions n×m. We can compute the matrix using the relationship T = X, where each column of W corresponds to an eigenvector of the matrix XΤX.\n",
    "\n",
    "While PCA has been used for decades for dimensionality reduction, it spectacularly fails to capture important relationships that are piecewise linear or nonlinear. Take, for instance, the example illustrated in Figure below.\n",
    "\n",
    "<img src='images/img3.PNG'>\n",
    "\n",
    "The example shows data points selected at random from two concentric circles. We hope that PCA will transform this dataset so that we can pick a single new axis that allows us to easily separate the red and blue dots. Unfortunately for us, there is no linear direction that contains more information here than another (we have equal variance in all directions). Instead, as a human being, we notice that information is being encoded in a nonlinear way, in terms of how far points are from the origin. With this information in mind, we notice that the polar transformation (expressing points as their distance from the origin, as the new horizontal axis, and their angle bearing from the original x-axis, as the new vertical axis) does just the trick. \n",
    "\n",
    "Above figure highlights the shortcomings of an approach like PCA in capturing important relationships in complex datasets. Because most of the datasets we are likely to encounter in the wild (images, text, etc.) are characterized by nonlinear relationships, we must develop a theory that will perform nonlinear dimensionality reduction. Deep learning practitioners have closed this gap using neural models, which we’ll cover in the next section."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing Models over the CPU and GPU\n",
    "\n",
    "TensorFlow allows us to utilize multiple computing devices, if we so desire, to build and train our models. Supported devices are represented by string IDs and normally consist of the following:\n",
    "\n",
    "* \"/cpu:0\" The CPU of this machine\n",
    "* \"/gpu:0\" The first GPU of this machine (If you've have a Nvidia GPU)\n",
    "* \"/gpu:1\" The second GPU of this machine.\n",
    "\n",
    "When a TensorFlow operation has both CPU and GPU kernels, and GPU use is enabled, TensorFlow will automatically opt to use the GPU implementation. To inspect which devices are used by the computational graph, we can initialize our TensorFlow session with the log_device_placement set to True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess=tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we desire to use a specific device, we may do so by using with tf.device to select the appropriate device. If the chosen device is not available, however, an error will be thrown. If we would like TensorFlow to find another available device if the chosen device does not exist, we can pass the allow_soft_placement flag to the session variable as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.],\n",
       "       [11.]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    a=tf.constant([1.0,2.0,3.0,4.0],shape=[2,2],name='a')\n",
    "    b=tf.constant([1.0,2.0],shape=[2,1],name='b')\n",
    "    c=tf.matmul(a,b)\n",
    "\n",
    "sess=tf.Session(config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=True))\n",
    "sess.run(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow also  allows us to build models that span multiple GPUs by building models in a tower-like fashion as shown in Figure below. The following code is an example of multi-GPU code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Cannot assign a device for operation 'MatMul': Operation was explicitly assigned to /device:GPU:2 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:GPU:0 ]. Make sure the device specification refers to a valid device.\n\t [[Node: MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/device:GPU:2\"](a_1, b)]]\n\nCaused by op 'MatMul', defined at:\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 497, in start\n    self.io_loop.start()\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\asyncio\\base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\asyncio\\base_events.py\", line 1426, in _run_once\n    handle._run()\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\asyncio\\events.py\", line 127, in _run\n    self._callback(*self._args)\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-6df6d562a2fd>\", line 4, in <module>\n    c=tf.matmul(a,b)\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 2014, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 4567, in mat_mul\n    name=name)\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation 'MatMul': Operation was explicitly assigned to /device:GPU:2 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:GPU:0 ]. Make sure the device specification refers to a valid device.\n\t [[Node: MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/device:GPU:2\"](a_1, b)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1304\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1305\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n",
      "\u001b[1;32mc:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1339\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1340\u001b[1;33m         \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1341\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Cannot assign a device for operation 'MatMul': Operation was explicitly assigned to /device:GPU:2 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:GPU:0 ]. Make sure the device specification refers to a valid device.\n\t [[Node: MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/device:GPU:2\"](a_1, b)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-15837979b26a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConfigProto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_device_placement\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1335\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1337\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Cannot assign a device for operation 'MatMul': Operation was explicitly assigned to /device:GPU:2 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:GPU:0 ]. Make sure the device specification refers to a valid device.\n\t [[Node: MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/device:GPU:2\"](a_1, b)]]\n\nCaused by op 'MatMul', defined at:\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 497, in start\n    self.io_loop.start()\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\asyncio\\base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\asyncio\\base_events.py\", line 1426, in _run_once\n    handle._run()\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\asyncio\\events.py\", line 127, in _run\n    self._callback(*self._args)\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-6df6d562a2fd>\", line 4, in <module>\n    c=tf.matmul(a,b)\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 2014, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 4567, in mat_mul\n    name=name)\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation 'MatMul': Operation was explicitly assigned to /device:GPU:2 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:GPU:0 ]. Make sure the device specification refers to a valid device.\n\t [[Node: MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/device:GPU:2\"](a_1, b)]]\n"
     ]
    }
   ],
   "source": [
    "c=[]\n",
    "sum_=0\n",
    "for d in ['/device:GPU:0','/device:GPU:1']:\n",
    "    with tf.device(d):\n",
    "        a = tf.constant([1.0, 2.0, 3.0, 4.0], shape=[2, 2],name='a')\n",
    "        b = tf.constant([1.0, 2.0], shape=[2, 1], name='b') \n",
    "        c.append(tf.matmul(a,b))\n",
    "with tf.device('/cpu:0'):\n",
    "    sum_=tf.add_n(c)\n",
    "\n",
    "sess=tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "sess.run(sum_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/image4.PNG'>\n",
    "\n",
    "# Specifying the Logistic Regression Model in TensorFlow\n",
    "\n",
    "Now that we’ve developed all of the basic concepts of TensorFlow, let’s build a simple model to tackle the MNIST dataset. As you may recall, our goal is to identify handwritten digits from 28 x 28 black-and-white images. The first network that we’ll build implements a simple machine learning algorithm known as logistic regression.\n",
    "\n",
    "On a high level, logistic regression is a method by which we can calculate the probability that an input belongs to one of the target classes. In our case, we’ll compute the probability that a given input image is a 0, 1, ..., or 9. Our model uses a matrix W representing the weights of the connections in the network, as well as a vector b corresponding to the biases to estimate whether an input x belongs to class i using the softmax expression we talked about earlier: \n",
    "\n",
    "<img src='images/softmax.PNG'>\n",
    "\n",
    "Our goal is to learn the values for W and b that most effectively classify our inputs as accurately as possible. Pictorially, we can express the logistic regression network as shown in Figure below (bias connections are not shown to reduce clutter).\n",
    "\n",
    "<img src='images/image5.PNG'>\n",
    "\n",
    "You’ll notice that the network interpretation for logistic regression is rather primitive. It doesn’t have any hidden layers, meaning that it is limited in its ability to learn complex relationships! We have an output softmax of size 10 because we have 10 possible outcomes for each input. Moreover, we have an input layer of size 784, one input neuron for every pixel in the image! As we’ll see, the model makes decent headway toward correctly classifying our dataset, but there’s lots of room for improvement. \n",
    "\n",
    "We’ll build the the logistic regression model in four phases:\n",
    "\n",
    "* 1. inference: produces a probability distribution over the output classes given a minibatch.\n",
    "\n",
    "* 2. loss: computes the value of the error function (in this case, the cross-entropy loss).\n",
    "\n",
    "* 3. training: responsible for computing the gradients of the model’s parameters and updating the model.\n",
    "\n",
    "* 4. evaluate: will determine the effectiveness of a model \n",
    "\n",
    "Given a minibatch, which consists of 784-dimensional vectors representing MNIST images, we can represent logistic regression by taking the softmax of the input multiplied with a matrix representing the weights connecting the input and output layer. Each row of the output tensor represents the probability distribution over output classes for each corresponding data sample in the minibatch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-38-c77b60311148>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data\\train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting data\\t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist=input_data.read_data_sets('data',one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(x):\n",
    "    tf.constant_initializer(value=0)\n",
    "    W=tf.get_variable(\"W\",[784,10])\n",
    "    b=tf.get_variable(\"b\",[10])\n",
    "    output=tf.nn.softmax(tf.matmul(x,W)+b)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, given the correct labels for a minibatch, we should be able to compute the average error per data sample. We accomplish this using the following code snippet that computes the cross-entropy loss    over a minibatch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(output,y):\n",
    "    dot_product=y*tf.log(output)\n",
    "    \n",
    "    ''' Reduction along axis 0 collapses each column into a \n",
    "     single value, whereas reduction along axis 1 collapses \n",
    "     each row into a single value. In general, reduction along\n",
    "     axis i collapses the ith dimension of a tensor to size 1.'''\n",
    "    x_entropy=-tf.reduce_sum(dot_product,reduction_indices=1)\n",
    "    loss=tf.reduce_mean(x_entropy)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, given the current cost incurred, we’ll want to compute the gradients and modify the parameters of the model appropriately. TensorFlow makes this easy by giving us access to built-in optimizers that produce a special train operation that we can run via a TensorFlow session when we minimize them. Note that when we create the training operation, we also pass in a variable that represents the number of minibatches that have been processed. Each time the training operation is run, this step variable is incremented so that we can keep track of progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(cost,global_step):\n",
    "    print(\"Cost: \",cost)\n",
    "    optimizer=tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    train_op=optimizer.minimize(cost,global_step=global_step)\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we put together a simple computational subgraph to evaluate the model on the validation or  test set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(output,y):\n",
    "    correct_prediction=tf.equal(tf.argmax(output,1),tf.argmax(y,1))\n",
    "    accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float64))\n",
    "    print(accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This completes Tensorflow graph setup for the logistic regression model.\n",
    "\n",
    "## Logging and Training the Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost:  Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "WARNING:tensorflow:Passing a `GraphDef` to the SummaryWriter is deprecated. Pass a `Graph` object instead, such as `sess.graph`.\n",
      "Validation Error:  0.13999998569488525\n",
      "Validation Error:  0.12000000476837158\n",
      "Validation Error:  0.11000001430511475\n",
      "Validation Error:  0.1600000262260437\n",
      "Validation Error:  0.12000000476837158\n",
      "Validation Error:  0.1899999976158142\n",
      "Validation Error:  0.07999998331069946\n",
      "Validation Error:  0.12999999523162842\n",
      "Validation Error:  0.07999998331069946\n",
      "Validation Error:  0.11000001430511475\n",
      "Validation Error:  0.11000001430511475\n",
      "Validation Error:  0.04000002145767212\n",
      "Validation Error:  0.07999998331069946\n",
      "Validation Error:  0.12000000476837158\n",
      "Validation Error:  0.04000002145767212\n",
      "Validation Error:  0.0899999737739563\n",
      "Validation Error:  0.050000011920928955\n",
      "Validation Error:  0.10000002384185791\n",
      "Validation Error:  0.11000001430511475\n",
      "Validation Error:  0.0899999737739563\n",
      "Validation Error:  0.04000002145767212\n",
      "Validation Error:  0.17000001668930054\n",
      "Validation Error:  0.11000001430511475\n",
      "Validation Error:  0.10000002384185791\n",
      "Validation Error:  0.12999999523162842\n",
      "Validation Error:  0.0899999737739563\n",
      "Validation Error:  0.13999998569488525\n",
      "Validation Error:  0.10000002384185791\n",
      "Validation Error:  0.06999999284744263\n",
      "Validation Error:  0.050000011920928955\n",
      "Validation Error:  0.04000002145767212\n",
      "Validation Error:  0.07999998331069946\n",
      "Validation Error:  0.11000001430511475\n",
      "Validation Error:  0.10000002384185791\n",
      "Validation Error:  0.10000002384185791\n",
      "Validation Error:  0.0899999737739563\n",
      "Validation Error:  0.10000002384185791\n",
      "Validation Error:  0.06999999284744263\n",
      "Validation Error:  0.11000001430511475\n",
      "Validation Error:  0.10000002384185791\n",
      "Validation Error:  0.11000001430511475\n",
      "Validation Error:  0.11000001430511475\n",
      "Validation Error:  0.06999999284744263\n",
      "Validation Error:  0.1499999761581421\n",
      "Validation Error:  0.07999998331069946\n",
      "Validation Error:  0.11000001430511475\n",
      "Validation Error:  0.06000000238418579\n",
      "Validation Error:  0.06999999284744263\n",
      "Validation Error:  0.0899999737739563\n",
      "Validation Error:  0.06999999284744263\n",
      "Validation Error:  0.04000002145767212\n",
      "Validation Error:  0.11000001430511475\n",
      "Validation Error:  0.1499999761581421\n",
      "Validation Error:  0.0899999737739563\n",
      "Validation Error:  0.07999998331069946\n",
      "Validation Error:  0.07999998331069946\n",
      "Validation Error:  0.050000011920928955\n",
      "Validation Error:  0.06999999284744263\n",
      "Validation Error:  0.050000011920928955\n",
      "Validation Error:  0.0899999737739563\n",
      "Validation Error:  0.04000002145767212\n",
      "Validation Error:  0.0899999737739563\n",
      "Validation Error:  0.12000000476837158\n",
      "Validation Error:  0.07999998331069946\n",
      "Validation Error:  0.04000002145767212\n",
      "Validation Error:  0.0899999737739563\n",
      "Validation Error:  0.07999998331069946\n",
      "Validation Error:  0.04000002145767212\n",
      "Validation Error:  0.11000001430511475\n",
      "Validation Error:  0.07999998331069946\n",
      "Validation Error:  0.06999999284744263\n",
      "Validation Error:  0.10000002384185791\n",
      "Validation Error:  0.10000002384185791\n",
      "Validation Error:  0.0899999737739563\n",
      "Validation Error:  0.009999990463256836\n",
      "Validation Error:  0.10000002384185791\n",
      "Validation Error:  0.06000000238418579\n",
      "Validation Error:  0.10000002384185791\n",
      "Validation Error:  0.07999998331069946\n",
      "Validation Error:  0.07999998331069946\n",
      "Validation Error:  0.050000011920928955\n",
      "Validation Error:  0.07999998331069946\n",
      "Validation Error:  0.06999999284744263\n",
      "Validation Error:  0.06000000238418579\n",
      "Validation Error:  0.07999998331069946\n",
      "Validation Error:  0.050000011920928955\n",
      "Validation Error:  0.12000000476837158\n",
      "Validation Error:  0.06000000238418579\n",
      "Validation Error:  0.10000002384185791\n",
      "Validation Error:  0.050000011920928955\n",
      "Validation Error:  0.06999999284744263\n",
      "Validation Error:  0.050000011920928955\n",
      "Validation Error:  0.12000000476837158\n",
      "Validation Error:  0.07999998331069946\n",
      "Validation Error:  0.0899999737739563\n",
      "Validation Error:  0.07999998331069946\n",
      "Validation Error:  0.029999971389770508\n",
      "Validation Error:  0.07999998331069946\n",
      "Validation Error:  0.04000002145767212\n",
      "Validation Error:  0.06000000238418579\n",
      "Operation finised\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "learning_rate=0.01\n",
    "training_epochs=100\n",
    "batch_size=100\n",
    "display_step=1\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    # mnist data image shape 28*28=784\n",
    "    x=tf.placeholder(\"float\",[None,784])\n",
    "    y=tf.placeholder(\"float\",[None,10])\n",
    "    output=inference(x)\n",
    "    \n",
    "    cost=loss(output,y)\n",
    "    global_step=tf.Variable(0,name='global_step',trainable=False)\n",
    "    train_op=training(cost,global_step)\n",
    "    eval_op=evaluate(output,y)\n",
    "    summary_op=tf.summary.merge_all()\n",
    "    saver=tf.train.Saver()\n",
    "    sess=tf.Session()\n",
    "    \n",
    "    summary_writer=tf.summary.FileWriter(\"logistic_logs/\",graph_def=sess.graph_def)\n",
    "    \n",
    "    init_op=tf.initialize_all_variables()\n",
    "    \n",
    "    sess.run(init_op)\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost=0\n",
    "        total_batch=int(mnist.train.num_examples/batch_size)\n",
    "        # loop over all batchs\n",
    "        for i in range(total_batch):\n",
    "            mbatch_x,mbatch_y=mnist.train.next_batch(batch_size)\n",
    "            # Fit training using batch data\n",
    "            feed_dict={x : mbatch_x,y : mbatch_y}\n",
    "            sess.run(train_op,feed_dict)\n",
    "            # Compute average loss\n",
    "            minibatch_cost = sess.run(cost,feed_dict)\n",
    "            avg_cost+=minibatch_cost/total_batch\n",
    "        # Displat logs per epoch step\n",
    "        if epoch%display_step==0:\n",
    "            val_feed_dict={\n",
    "                x:mnist.validation.images,\n",
    "                y:mnist.validation.labels\n",
    "            }\n",
    "            accuracy=sess.run(eval_op,feed_dict)\n",
    "            print(\"Validation Error: \",(1-accuracy))\n",
    "            #summary_str=sess.run(summary_op,feed_dict=feed_dict)\n",
    "            saver.save(sess,\"logistic_logs/model-checkpoint\",global_step)\n",
    "        \n",
    "print(\"Operation finised\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard Visualization\n",
    "\n",
    "<img src='images/tensorboard.PNG'>\n",
    "\n",
    "## Building a Multilayer Model for MNIST\n",
    "\n",
    "<img src='images/image6.PNG'>\n",
    "\n",
    "We can reuse most of the code from our logistic regression example with a couple of modifications:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer(input,weight_shape,bias_shape):\n",
    "    print(input)\n",
    "    weight_stddev=(2.0/weight_shape[0])**0.5\n",
    "    w_init=tf.random_normal_initializer(stddev=weight_stddev)\n",
    "    bias_init=tf.constant_initializer(value=0)\n",
    "    W=tf.get_variable(\"W\",weight_shape,initializer=w_init,dtype=tf.float32)\n",
    "    b=tf.get_variable(\"b\",bias_shape,initializer=bias_init,dtype=tf.float32)\n",
    "    return tf.nn.relu(tf.matmul(a=input,b=W)+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(x):\n",
    "    with tf.variable_scope(\"hidden_1\"):\n",
    "        hidden_1=layer(x,[784,256],[256])\n",
    "    \n",
    "    with tf.variable_scope(\"hidden_2\"):\n",
    "        hidden_2=layer(hidden_1,[256,256],[256])        \n",
    "    \n",
    "    with tf.variable_scope(\"output\"):\n",
    "        output=layer(hidden_2,[256,10],[10])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the new code is self explanatory, but our initialization strategy deserves some additional description. The performance of deep neural networks very much depends on an effective initialization of its parameters. There are many features of the error surfaces of deep neural networks that make optimization using vanilla stochastic gradient descent very difficult. This problem is exacerbated as the number of layers in the model (and thus the complexity of the error surface) increases. Smart initialization is one way to mitigate this issue. \n",
    "\n",
    "For ReLU units, a study published in 2015 by He et al. demonstrates that the variance of weights in a network should be $ \\frac{2}{n_{in}} $ where $n_{in}$ is the number of inputs coming into the neuron. The curious reader should investigate what happens when we change our initialization strategy. For example, changing tf.random_nor mal_initializer back to the tf.random_uniform_initializer we used in the logistic regression example significantly hurts performance. \n",
    "\n",
    "Finally, for slightly better performance, we perform the softmax while computing the loss instead of during the inference phase of the network. This results in the following modification:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(output,y):\n",
    "    xentropy=tf.nn.softmax_cross_entropy_with_logits(logits=output,labels=y)\n",
    "    loss=tf.reduce_mean(xentropy)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this program for 300 epochs gives us a massive improvement over the logistic regression model. The model operates with an accuracy of 98.2%, which is nearly a 78% reduction in the per-digit error rate compared to our first attempt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(?, 784), dtype=float32)\n",
      "Tensor(\"hidden_1/Relu:0\", shape=(?, 256), dtype=float32)\n",
      "Tensor(\"hidden_2/Relu:0\", shape=(?, 256), dtype=float32)\n",
      "Cost:  Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "Tensor(\"Mean_1:0\", shape=(), dtype=float32)\n",
      "WARNING:tensorflow:Passing a `GraphDef` to the SummaryWriter is deprecated. Pass a `Graph` object instead, such as `sess.graph`.\n",
      "Validation Error:  0.2890625  Accuracy:  0.7109375\n",
      "Validation Error:  0.1953125  Accuracy:  0.8046875\n",
      "Validation Error:  0.2265625  Accuracy:  0.7734375\n",
      "Validation Error:  0.1796875  Accuracy:  0.8203125\n",
      "Validation Error:  0.1953125  Accuracy:  0.8046875\n",
      "Validation Error:  0.1484375  Accuracy:  0.8515625\n",
      "Validation Error:  0.1640625  Accuracy:  0.8359375\n",
      "Validation Error:  0.0625  Accuracy:  0.9375\n",
      "Validation Error:  0.0546875  Accuracy:  0.9453125\n",
      "Validation Error:  0.0546875  Accuracy:  0.9453125\n",
      "Validation Error:  0.078125  Accuracy:  0.921875\n",
      "Validation Error:  0.0625  Accuracy:  0.9375\n",
      "Validation Error:  0.0390625  Accuracy:  0.9609375\n",
      "Validation Error:  0.0234375  Accuracy:  0.9765625\n",
      "Validation Error:  0.0546875  Accuracy:  0.9453125\n",
      "Validation Error:  0.046875  Accuracy:  0.953125\n",
      "Validation Error:  0.0546875  Accuracy:  0.9453125\n",
      "Validation Error:  0.0234375  Accuracy:  0.9765625\n",
      "Validation Error:  0.0703125  Accuracy:  0.9296875\n",
      "Validation Error:  0.0234375  Accuracy:  0.9765625\n",
      "Validation Error:  0.0390625  Accuracy:  0.9609375\n",
      "Validation Error:  0.0  Accuracy:  1.0\n",
      "Validation Error:  0.03125  Accuracy:  0.96875\n",
      "Validation Error:  0.03125  Accuracy:  0.96875\n",
      "Validation Error:  0.046875  Accuracy:  0.953125\n",
      "Validation Error:  0.0234375  Accuracy:  0.9765625\n",
      "Validation Error:  0.03125  Accuracy:  0.96875\n",
      "Validation Error:  0.0234375  Accuracy:  0.9765625\n",
      "Validation Error:  0.0390625  Accuracy:  0.9609375\n",
      "Validation Error:  0.0546875  Accuracy:  0.9453125\n",
      "Validation Error:  0.0234375  Accuracy:  0.9765625\n",
      "Validation Error:  0.046875  Accuracy:  0.953125\n",
      "Validation Error:  0.046875  Accuracy:  0.953125\n",
      "Validation Error:  0.015625  Accuracy:  0.984375\n",
      "Validation Error:  0.0234375  Accuracy:  0.9765625\n",
      "Validation Error:  0.0390625  Accuracy:  0.9609375\n",
      "Validation Error:  0.0234375  Accuracy:  0.9765625\n",
      "Validation Error:  0.03125  Accuracy:  0.96875\n",
      "Validation Error:  0.03125  Accuracy:  0.96875\n",
      "Validation Error:  0.0234375  Accuracy:  0.9765625\n",
      "Validation Error:  0.03125  Accuracy:  0.96875\n",
      "Validation Error:  0.0078125  Accuracy:  0.9921875\n",
      "Validation Error:  0.03125  Accuracy:  0.96875\n",
      "Validation Error:  0.015625  Accuracy:  0.984375\n",
      "Validation Error:  0.015625  Accuracy:  0.984375\n",
      "Validation Error:  0.0234375  Accuracy:  0.9765625\n",
      "Validation Error:  0.0078125  Accuracy:  0.9921875\n",
      "Validation Error:  0.03125  Accuracy:  0.96875\n",
      "Validation Error:  0.0078125  Accuracy:  0.9921875\n",
      "Validation Error:  0.0078125  Accuracy:  0.9921875\n",
      "Validation Error:  0.0234375  Accuracy:  0.9765625\n",
      "Validation Error:  0.0234375  Accuracy:  0.9765625\n",
      "Validation Error:  0.0234375  Accuracy:  0.9765625\n",
      "Validation Error:  0.0078125  Accuracy:  0.9921875\n",
      "Validation Error:  0.046875  Accuracy:  0.953125\n",
      "Validation Error:  0.015625  Accuracy:  0.984375\n",
      "Validation Error:  0.03125  Accuracy:  0.96875\n",
      "Validation Error:  0.03125  Accuracy:  0.96875\n",
      "Validation Error:  0.015625  Accuracy:  0.984375\n",
      "Validation Error:  0.0078125  Accuracy:  0.9921875\n",
      "Validation Error:  0.0078125  Accuracy:  0.9921875\n",
      "Validation Error:  0.0  Accuracy:  1.0\n",
      "Validation Error:  0.015625  Accuracy:  0.984375\n",
      "Validation Error:  0.015625  Accuracy:  0.984375\n",
      "Validation Error:  0.0078125  Accuracy:  0.9921875\n",
      "Validation Error:  0.0078125  Accuracy:  0.9921875\n",
      "Validation Error:  0.015625  Accuracy:  0.984375\n",
      "Validation Error:  0.0  Accuracy:  1.0\n",
      "Validation Error:  0.0234375  Accuracy:  0.9765625\n",
      "Validation Error:  0.0  Accuracy:  1.0\n",
      "Validation Error:  0.015625  Accuracy:  0.984375\n",
      "Validation Error:  0.015625  Accuracy:  0.984375\n",
      "Validation Error:  0.015625  Accuracy:  0.984375\n",
      "Validation Error:  0.0  Accuracy:  1.0\n",
      "Validation Error:  0.0078125  Accuracy:  0.9921875\n",
      "Validation Error:  0.0234375  Accuracy:  0.9765625\n",
      "Validation Error:  0.015625  Accuracy:  0.984375\n",
      "Validation Error:  0.0078125  Accuracy:  0.9921875\n",
      "Validation Error:  0.0234375  Accuracy:  0.9765625\n",
      "Validation Error:  0.0  Accuracy:  1.0\n",
      "Validation Error:  0.0  Accuracy:  1.0\n",
      "Validation Error:  0.0  Accuracy:  1.0\n",
      "Validation Error:  0.015625  Accuracy:  0.984375\n",
      "Validation Error:  0.0  Accuracy:  1.0\n",
      "Validation Error:  0.0078125  Accuracy:  0.9921875\n",
      "Validation Error:  0.0  Accuracy:  1.0\n",
      "Validation Error:  0.0  Accuracy:  1.0\n",
      "Validation Error:  0.015625  Accuracy:  0.984375\n",
      "Validation Error:  0.0078125  Accuracy:  0.9921875\n",
      "Validation Error:  0.0  Accuracy:  1.0\n",
      "Validation Error:  0.0  Accuracy:  1.0\n",
      "Validation Error:  0.0  Accuracy:  1.0\n",
      "Validation Error:  0.015625  Accuracy:  0.984375\n",
      "Validation Error:  0.0078125  Accuracy:  0.9921875\n",
      "Validation Error:  0.0  Accuracy:  1.0\n",
      "Validation Error:  0.0  Accuracy:  1.0\n",
      "Validation Error:  0.0078125  Accuracy:  0.9921875\n",
      "Validation Error:  0.0078125  Accuracy:  0.9921875\n",
      "Validation Error:  0.015625  Accuracy:  0.984375\n",
      "Validation Error:  0.0078125  Accuracy:  0.9921875\n",
      "Operation finised\n"
     ]
    }
   ],
   "source": [
    "learning_rate=0.01\n",
    "training_epochs=100\n",
    "batch_size=128\n",
    "display_step=1\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    # mnist data image shape 28*28=784\n",
    "    x=tf.placeholder(tf.float32,[None,784])\n",
    "    y=tf.placeholder(tf.float32,[None,10])\n",
    "    output=inference(x)\n",
    "    \n",
    "    cost=loss(output,y)\n",
    "    global_step=tf.Variable(0,name='global_step',trainable=False)\n",
    "    train_op=training(cost,global_step)\n",
    "    eval_op=evaluate(output,y)\n",
    "    summary_op=tf.summary.merge_all()\n",
    "    saver=tf.train.Saver()\n",
    "    sess=tf.Session()\n",
    "    \n",
    "    summary_writer=tf.summary.FileWriter(\"logistic_logs/\",graph_def=sess.graph_def)\n",
    "    \n",
    "    init_op=tf.initialize_all_variables()\n",
    "    \n",
    "    sess.run(init_op)\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost=0\n",
    "        total_batch=int(mnist.train.num_examples/batch_size)\n",
    "        # loop over all batchs\n",
    "        for i in range(total_batch):\n",
    "            mbatch_x,mbatch_y=mnist.train.next_batch(batch_size)\n",
    "            # Fit training using batch data\n",
    "            feed_dict={x : mbatch_x,y : mbatch_y}\n",
    "            sess.run(train_op,feed_dict)\n",
    "            # Compute average loss\n",
    "            minibatch_cost = sess.run(cost,feed_dict)\n",
    "            avg_cost+=minibatch_cost/total_batch\n",
    "        # Displat logs per epoch step\n",
    "        if epoch%display_step==0:\n",
    "            val_feed_dict={\n",
    "                x:mnist.validation.images,\n",
    "                y:mnist.validation.labels\n",
    "            }\n",
    "            accuracy=sess.run(eval_op,feed_dict)\n",
    "            print(\"Validation Error: \",(1-accuracy),\" Accuracy: \",accuracy)\n",
    "            #summary_str=sess.run(summary_op,feed_dict=feed_dict)\n",
    "            saver.save(sess,\"logistic_logs/model-checkpoint\",global_step)\n",
    "        \n",
    "print(\"Operation finised\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

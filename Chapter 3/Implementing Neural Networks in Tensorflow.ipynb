{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Tensorflow?\n",
    "\n",
    "TensorFlow is a Python library that allows users to express arbitrary computation as a graph of data flows. Nodes in this graph represent mathematical operations, whereas edges represent data that is communicated from one node to another. Data in TensorFlow is represented as tensors, which are multidimensional arrays (representing vectors with a 1D tensor, matrices with a 2D tensor, etc.).\n",
    "\n",
    "## Installing Tensorflow\n",
    "\n",
    "$ pip install --upgrade tensorflow      # for Python 2.7 \n",
    "$ pip3 install --upgrade tensorflow     # for Python 3.n\n",
    "$ pip install --upgrade tensorflow-gpu  # for Python 2.7  and GPU \n",
    "$ pip3 install --upgrade tensorflow-gpu # for Python 3.n  and GPU \n",
    "\n",
    "* Note: Do not forget to install CUDA for tensorflow GPU if you have a Nvidia GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Testing tensorflow'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "dl=tf.constant('Testing tensorflow')\n",
    "session=tf.Session()\n",
    "session.run(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=tf.constant(2)\n",
    "b=tf.constant(3)\n",
    "multiply=tf.multiply(a,b)\n",
    "session.run(multiply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add=tf.add(a,b) #add\n",
    "session.run(add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Manipulating Tensorflow Variables\n",
    "\n",
    "When we build a deep learning model in TensorFlow, we use variables to represent the parameters of the model. TensorFlow variables are in-memory buffers that contain tensors; but unlike normal tensors that are only instantiated when a graph is run and that are immediately wiped clean afterward, variables survive across multiple executions of a graph. As a result, TensorFlow variables have the following three properties:\n",
    "\n",
    "* Variables must be explicitly initialized before a graph is used for the first time.\n",
    "\n",
    "* We can use gradient methods to modify variables after each iteration as we search for a model’s optimal parameter settings.\n",
    "\n",
    "* We can save the values stored in variables to disk and restore them for later use. \n",
    "\n",
    "These three properties are what make TensorFlow especially useful for building machine learning models. \n",
    "\n",
    "Creating a variable is simple, and TensorFlow provides mechanics that allow us to initialize variables in several ways. Let’s start off by initializing a variable that describes the weights connecting neurons between two layers of a feed-forward neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "weigths=tf.Variable(tf.random_normal([300,200],stddev=0.5),name='weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we pass two arguments  to tf.Variable.6 The first, tf.random_normal,7 is  an operation that produces a tensor initialized using a normal distribution with standard deviation 0.5. We’ve specified that this tensor is of size 300 x 200, implying that the weights connect a layer with 300 neurons to a layer with 200 neurons. We’ve also passed a name to our call to tf.Variable.  The name is a unique identifier that allows us to refer to the appropriate node in the computation graph. In this case, weights is meant to be trainable; or in other words, we will automatically compute and apply gradients to weights. If weights is not meant to be trainable, we may pass an optional flag when we call tf.Variable:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weigths=tf.Variable(tf.random_normal([300,200],stddev=0.5),name='weights',trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to using tf.random_normal, there are several other methods to initialize a TensorFlow variable:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'zeros:0' shape=(100, 50) dtype=float32>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# common tensors from the Tensorflow API docs\n",
    "tf.zeros([100,50],dtype=tf.float32,name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ones:0' shape=(100, 50) dtype=float32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.ones([100,50],dtype=tf.float32,name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'random_normal_2:0' shape=(100, 50) dtype=float32>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random_normal([100,50],mean=0,stddev=1.0,dtype=tf.float32,seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'truncated_normal:0' shape=(100, 50) dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.truncated_normal([100,50],mean=0,stddev=1.0,dtype=tf.float32,seed=None,name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'random_uniform:0' shape=(100, 50) dtype=float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random_uniform([100,50],minval=0,maxval=None,dtype=tf.float32,seed=None,name=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we call tf.Variable, three operations are added to the computation graph: \n",
    "\n",
    "* The operation producing the tensor we use to initialize our variable.\n",
    "* The tf.assign operation, which is responsible for filling the variable with the initializing tensor prior to the variable’s use.\n",
    "* The variable operation, which holds the current value of the variable.\n",
    "\n",
    "This can be visualized as shown\n",
    "<img src='images/image1.PNG'>\n",
    "\n",
    "As we mentioned previously in the three operations, before we use any TensorFlow variable, the tf.assign operation must be run so that the variable is appropriately initialized with the desired value. We can do this by running tf.initial ize_all_variables(), which will trigger all of the tf.assign operations in our graph. We can also selectively initialize only certain variables in our computational graph using the tf.initialize_variables(var1, var2, ...). We’ll describe this in more detail when we discuss sessions in TensorFlow.\n",
    "\n",
    "\n",
    "## Tensorflow Operations\n",
    "\n",
    "We’ve already talked a little bit about operations in the context of variable initialization, but these only make up a small subset of the universe of operations available in TensorFlow. On a high level, TensorFlow operations represent abstract transformations that are applied to tensors in the computation graph. Operations may have attributes that may be supplied a priori or are inferred at runtime. For example, an attribute may serve to describe the expected types of input (adding tensors of type float32 versus int32). Just as variables are named, operations may also be supplied with an optional name attribute for easy reference into the computation graph.\n",
    "\n",
    "An operation consists of one or more kernels, which represent device-specific implementations. For example, an operation may have separate CPU and GPU kernels because it can be more efficiently expressed on a GPU. This is the case for many TensorFlow operations on matrices.\n",
    "\n",
    "<img src='images/image2.PNG'>\n",
    "\n",
    "\n",
    "## Placeholder Tensors\n",
    "\n",
    "Now that we have a solid understanding of TensorFlow variables and operations, we have a nearly complete description of the components of a TensorFlow computation graph. The only missing piece is how we pass the input to our deep model (during both train and test time). A variable is insufficient because it is only meant to be initialized once. Instead, we need a component that we populate every single time the computation graph is run.\n",
    "\n",
    "TensorFlow solves this problem using a construct called a placeholder. A placeholder is instantiated as follows and can be used in operations just like ordinary TensorFlow variables and tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.placeholder(tf.float32,name='x',shape=[None,784])\n",
    "W=tf.Variable(tf.random_uniform([784,10],-1,1),name='W')\n",
    "multiply=tf.matmul(x,W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we  define a placeholder where x represents a minibatch of data stored as float32’s. We notice that x has 784 columns, which means that each data sample has 784 dimensions. We also notice that x has an undefined number of rows. This means that x can be initialized with an arbitrary number of data samples. While we could instead multiply each data sample separately by W, expressing a full minibatch as a tensor allows us to compute the results for all the data samples in parallel. The result is that the ith row of the multiply tensor corresponds to W multiplied with the ith data sample. \n",
    "\n",
    "Just as variables need to be initialized the first time the computation graph is built, placeholders need to be filled every time the computation graph (or a subgraph) is run. We’ll discuss how this works in more detail in the next section.\n",
    "\n",
    "## Sessions in Tensorflow\n",
    "\n",
    "A TensorFlow program interacts with a computation graph using a session.13 The TensorFlow session is responsible for building the initial graph, and can be used to initialize all variables appropriately and to run the computational graph. To explore each of these pieces, let’s consider the following simple Python script:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "x=tf.placeholder(tf.float32,name='x',shape=[None,784])\n",
    "W=tf.Variable(tf.random_uniform([784,10],-1,1),name='W')\n",
    "b=tf.Variable(tf.zeros([10]),name='biases')\n",
    "output=tf.matmul(x,W)+b\n",
    "\n",
    "init_op=tf.initialize_all_variables()\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first four lines after the import statement describe the computational graph that is built by the session when it is finally instantiated. The graph (sans variable initialization operations) is depicted in Figure. We then initialize the variables as required by using the session variable to run the initialization operation in sess.run(init_op). Finally, we can run the subgraph by calling sess.run again, but this time we pass in the tensors (or list of tensors) we want to compute.\n",
    "\n",
    "<img src='images/image3.PNG'>\n",
    "\n",
    "Finally, the sess.run interface can also be used to train networks. We will explore this in further detail when we use TensorFlow to train our first machine learning model on MNIST. But how exactly does a single line of code (sess.run) accomplish such a wide variety of functions? The answer lies in the powerful expressivity of the underlying computational graph. \n",
    "\n",
    "All of these functionalities are represented as TensorFlow operations that can be passed as arguments to sess.run. All sess.run needs to do is traverse down the computational graph to identify all of the dependencies that compose the relevant subgraph, ensure that all of the placeholder variables that belong to the identified subgraph are filled using the feed_dict, and then traverse back up the subgraph (executing all of the intermediate operations) to evaluate the original arguments. Now that we have a comprehensive understanding of sessions and how to run them, we’ll explore two more major concepts in building and maintaining computational graphs.\n",
    "\n",
    "## Navigating Variable Scope and Sharing Variables\n",
    "\n",
    "Although we won’t run into this problem just yet, building complex models often requires reusing and sharing large sets of variables that we’ll want to instantiate together in one place. Unfortunately, trying to enforce modularity and readability can result in unintended results if we aren’t careful. Let’s consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_network(input):\n",
    "    W_1=tf.Variable(tf.random_uniform([784,100],-1,1),name='W_1')\n",
    "    b_1=tf.Variable(tf.zeros([100]),name='biases_1')\n",
    "    output_1=tf.matmul(input,W_1)+b_1\n",
    "    \n",
    "    W_2=tf.Variable(tf.random_uniform([100,50],-1,-1),name='W_2')\n",
    "    b_2=tf.Variable(tf.zeros([50]),name='biases_2')\n",
    "    output_2=tf.matmul(output_1,W_2)+b_2\n",
    "    \n",
    "    W_3=tf.Variable(tf.random_uniform([50,10],-1,1),name='W_3')\n",
    "    b_3=tf.Variable(tf.zeros([10]),name='biases_3')\n",
    "    output_3=tf.matmul(output_2,W_3)+b_3\n",
    "    \n",
    "    # printing names\n",
    "    print(\"Printing names of weight parameters\")\n",
    "    print(W_1.name,W_2.name,W_3.name)\n",
    "    print(\"Printing names of biases parameters\")\n",
    "    print(b_1.name,b_2.name,b_3.name)\n",
    "    \n",
    "    return output_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing names of weight parameters\n",
      "W_1_1:0 W_2:0 W_3:0\n",
      "Printing names of biases parameters\n",
      "biases_1:0 biases_2:0 biases_3:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_4:0' shape=(1000, 10) dtype=float32>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_1=tf.placeholder(tf.float32,[1000,784],name='i_1')\n",
    "my_network(i_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing names of weight parameters\n",
      "W_1_2:0 W_2_1:0 W_3_1:0\n",
      "Printing names of biases parameters\n",
      "biases_1_1:0 biases_2_1:0 biases_3_1:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_7:0' shape=(1000, 10) dtype=float32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_2=tf.placeholder(tf.float32,[1000,784],name='i_2')\n",
    "my_network(i_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we observe closely, our second call to my_network doesn’t use the same variables as the first call (in fact, the names are different!). Instead, we’ve created a second set of variables! In many cases, we don’t want to create a copy, but rather reuse the model and its variables. It turns out, that in this case, we shouldn’t be using tf.Variable. Instead, we should be using a more advanced naming scheme that takes advantage of TensorFlow’s variable scoping.\n",
    "\n",
    "TensorFlow’s variable scoping mechanisms are largely controlled by two functions:\n",
    "\n",
    "* tf.get_variable(name, shape, initializer) Checks if a variable with this name exists, retrieves the variable if it does, or creates it using the shape and initializer if it doesn’t.\n",
    "\n",
    "    \n",
    "* tf.variable_scope(scope_name) Manages the namespace and determines the scope in which tf.get_variable operates.\n",
    "\n",
    "Let’s try to rewrite my_network in a cleaner fashion using TensorFlow variable scoping. The new names of our variables are namespaced as \"layer1/W\", \"layer2/b\", \"layer2/W\", and so forth:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer(input,weight_shape,bias_shape):\n",
    "    weight_init=tf.random_uniform_initializer(minval=1,maxval=1)\n",
    "    bias_init=tf.constant_initializer(value=0)\n",
    "    W=tf.get_variable('W',weight_shape,initializer=weight_init)\n",
    "    b=tf.get_variable('b',bias_shape,initializer=bias_init)\n",
    "    return tf.matmul(input,W)+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_network(input):\n",
    "    with tf.variable_scope('layer_1'):\n",
    "        output_1=layer(input,[784,100],[100])\n",
    "    with tf.variable_scope('layer_2'):\n",
    "        output_2=layer(output_1,[100,50],[50])\n",
    "    with tf.variable_scope('layer_3'):\n",
    "        output_3=layer(output_2,[50,10],[10])\n",
    "    return output_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'layer_3/add:0' shape=(100, 10) dtype=float32>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let’s try to call my_network twice, just like we did in the preceding code block:\n",
    "\n",
    "i_1=tf.placeholder(tf.float32,[100,784],name='i_1')\n",
    "my_network(i_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable layer_1/W already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-936f70517254>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mi_2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'i_2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmy_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-8620bd3b1f3a>\u001b[0m in \u001b[0;36mmy_network\u001b[1;34m(input)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmy_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'layer_1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0moutput_1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'layer_2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0moutput_2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-21eb31bf8195>\u001b[0m in \u001b[0;36mlayer\u001b[1;34m(input, weight_shape, bias_shape)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mweight_init\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_uniform_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmaxval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mbias_init\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mW\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'W'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mweight_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweight_init\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'b'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbias_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias_init\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m   1326\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1328\u001b[1;33m       constraint=constraint)\n\u001b[0m\u001b[0;32m   1329\u001b[0m get_variable_or_local_docstring = (\n\u001b[0;32m   1330\u001b[0m     \"\"\"%s\n",
      "\u001b[1;32mc:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m   1088\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1089\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1090\u001b[1;33m           constraint=constraint)\n\u001b[0m\u001b[0;32m   1091\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1092\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32mc:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m    433\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m           \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m           constraint=constraint)\n\u001b[0m\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[1;32mc:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[0;32m    402\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[0;32m    405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[0;32m    741\u001b[0m                          \u001b[1;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[1;32m--> 743\u001b[1;33m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[0;32m    744\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Variable layer_1/W already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"c:\\users\\deepblue\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n"
     ]
    }
   ],
   "source": [
    "i_2=tf.placeholder(tf.float32,[1000,784],name='i_2')\n",
    "my_network(i_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike tf.Variable, the tf.get_variable command checks that a variable of the given name hasn’t already been instantiated. By default, sharing is not allowed (just to be safe!), but if we want to enable sharing within a variable scope, we can say so explicitly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"shared_variables\") as scope:\n",
    "    i_1=tf.placeholder(tf.float32,[1000,784],name=\"i_1\")\n",
    "    my_network(i_1)\n",
    "    scope.reuse_variables()\n",
    "    i_2= tf.placeholder(tf.float32,[1000,784],name=\"i_2\")\n",
    "    my_network(i_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows us to retain modularity while still allowing variable sharing. And as a nice byproduct, our naming scheme is cleaner as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
